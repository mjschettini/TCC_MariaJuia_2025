{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Código para incorporar palavras chaves ao dataset keywords"
      ],
      "metadata": {
        "id": "TANd1p3izYGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar o drive para importar o arquivo das pastas\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjXVtXFc69ee",
        "outputId": "d8301c14-1249-4f03-c5dc-8a5439e36e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilização do spacy para PNL a partir de verbos e substantivos agrupando as palavras\n",
        "Palavras iguais, mas em tempos diferentes"
      ],
      "metadata": {
        "id": "KH_MrkeUpSOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import os\n",
        "from collections import Counter\n",
        "import csv"
      ],
      "metadata": {
        "id": "c4J08tzP8Pk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o modelo de inglês\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Caminho do arquivo\n",
        "FILE_US = \"/content/drive/My Drive/TCC 2025/Base_de_dados/all_user_stories.txt\"\n",
        "\n",
        "# Percentual de histórias que serão processadas (0 a 100)\n",
        "PERCENTUAL_LIDAS = 80\n",
        "\n",
        "# Lista de palavras que devem ser ignoradas (estrutura da história de usuário)\n",
        "PALAVRAS_IGNORADAS = {\"as\", \"a\", \"i\", \"want\", \"so\", \"that\"}\n",
        "\n",
        "# Lista de verbos auxiliares válidos\n",
        "AUXILIARES = {\n",
        "    \"be\", \"am\", \"is\", \"are\", \"was\", \"were\",\n",
        "    \"do\", \"does\", \"did\",\n",
        "    \"have\", \"has\", \"had\",\n",
        "    \"will\", \"would\", \"shall\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\", \"need\"\n",
        "}\n",
        "\n",
        "# Carregar histórias de usuário\n",
        "def carregar_historias(filepath, percentual):\n",
        "    if os.path.exists(filepath):\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as arquivo:\n",
        "            todas = []\n",
        "            for linha in arquivo.readlines():\n",
        "                linha = linha.strip()\n",
        "                # # Divide na primeira vírgula e pega a parte após ela\n",
        "                # if \",\" in linha:\n",
        "                #     parte_util = linha.split(\",\", 1)[1].strip()\n",
        "                #     todas.append(parte_util)\n",
        "                # else:\n",
        "                todas.append(linha)  # mantém linha inteira se não tiver vírgula\n",
        "\n",
        "            qtd_lidas = max(1, int(len(todas) * percentual / 100))\n",
        "            historias_lidas = todas[:qtd_lidas]\n",
        "            historias_nao_lidas = todas[qtd_lidas:]\n",
        "            return historias_lidas, historias_nao_lidas\n",
        "    else:\n",
        "        print(f\"Arquivo {filepath} não encontrado.\")\n",
        "        return [], []\n",
        "\n",
        "\n",
        "historias_lidas, historias_nao_lidas = carregar_historias(FILE_US, PERCENTUAL_LIDAS)\n",
        "\n",
        "\n",
        "verbos_principais = []\n",
        "substantivos = []\n",
        "historias_com_verbos_substantivos = []\n",
        "\n",
        "# Processar cada história de usuário individualmente\n",
        "for historia in historias_lidas:\n",
        "    doc = nlp(historia) # Monta o doc da frase, com as informações que o spacy processa de cada token\n",
        "\n",
        "\n",
        "    verbos_historia_principal = []\n",
        "    substantivos_historia = []\n",
        "\n",
        "    for token in doc:\n",
        "        token_lower = token.text.lower()\n",
        "\n",
        "        # Ignorar palavras da estrutura padrão\n",
        "        if token_lower in PALAVRAS_IGNORADAS:\n",
        "            continue\n",
        "\n",
        "        if token.pos_ == \"VERB\":\n",
        "            verbo_base = token.lemma_.lower()\n",
        "\n",
        "            # Se não for auxiliar válido, considera como verbo principal\n",
        "            if verbo_base not in AUXILIARES:\n",
        "                verbos_principais.append(verbo_base)\n",
        "                verbos_historia_principal.append(verbo_base)\n",
        "\n",
        "        elif token.pos_ == \"NOUN\":\n",
        "            substantivo_base = token.lemma_.lower()\n",
        "            substantivos_historia.append(substantivo_base)\n",
        "            substantivos.append(substantivo_base)\n",
        "\n",
        "    historias_com_verbos_substantivos.append({\n",
        "        \"historia\": historia,\n",
        "        \"verbos_principais\": verbos_historia_principal,\n",
        "        \"substantivos\": substantivos_historia\n",
        "    })\n",
        "\n",
        "# Contar a frequência e ordenar por frequência decrescente\n",
        "contador_verbos_principais = Counter(verbos_principais).most_common()\n",
        "contador_substantivos = Counter(substantivos).most_common()\n",
        "\n",
        "# Caminho do arquivo de saída\n",
        "CSV_SAIDA = \"/content/drive/My Drive/TCC 2025/Base_de_dados/saida_palavras_sprint_final.csv\"\n",
        "\n",
        "# Combinar os dados em uma lista de tuplas (palavra, frequência, tipo)\n",
        "dados_csv = []\n",
        "\n",
        "for verbo, count in contador_verbos_principais:\n",
        "    dados_csv.append((verbo, count, \"V\"))\n",
        "\n",
        "for substantivo, count in contador_substantivos:\n",
        "    dados_csv.append((substantivo, count, \"N\"))\n",
        "\n",
        "# Escrever no arquivo CSV\n",
        "with open(CSV_SAIDA, mode=\"w\", newline=\"\", encoding=\"utf-8\") as arquivo_csv:\n",
        "    escritor = csv.writer(arquivo_csv)\n",
        "    escritor.writerow([\"palavra\", \"frequência\", \"tipo\"])  # cabeçalho\n",
        "    escritor.writerows(dados_csv)\n",
        "\n",
        "print(f\"\\nArquivo CSV salvo em: {CSV_SAIDA}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wYWnxlFpQCo",
        "outputId": "55be291d-ac76-4123-917a-b84a13d8eba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Arquivo CSV salvo em: /content/drive/My Drive/TCC 2025/Base_de_dados/saida_palavras_agrupadas_sprint_final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Caminho do arquivo de saída TXT para as histórias não utilizadas\n",
        "TXT_NAO_LIDAS = \"/content/drive/My Drive/TCC 2025/Base_de_dados/historias_nao_utilizadas_sprint_final.txt\"\n",
        "\n",
        "# Escrever as histórias não utilizadas no arquivo TXT\n",
        "if historias_nao_lidas:\n",
        "    with open(TXT_NAO_LIDAS, mode=\"w\", encoding=\"utf-8\") as arquivo_txt:\n",
        "        for historia in historias_nao_lidas:\n",
        "            arquivo_txt.write(historia + \"\\n\")\n",
        "    print(f\"Arquivo TXT com as histórias não utilizadas salvo em: {TXT_NAO_LIDAS}\")\n",
        "else:\n",
        "    print(\"Não há histórias de usuário restantes para salvar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqGxLPXq3p2f",
        "outputId": "1c92ffab-a806-476c-e7e5-4893e409ef52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo TXT com as histórias não utilizadas salvo em: /content/drive/My Drive/TCC 2025/Base_de_dados/historias_nao_utilizadas_sprint_final.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho do arquivo de saída TXT para as histórias lidas\n",
        "TXT_LIDAS = \"/content/drive/My Drive/TCC 2025/Base_de_dados/historias_utilizadas_sprint_final.txt\"\n",
        "\n",
        "# Escrever as histórias lidas no arquivo TXT\n",
        "if historias_lidas:\n",
        "    with open(TXT_LIDAS, mode=\"w\", encoding=\"utf-8\") as arquivo_txt_lidas:\n",
        "        for historia in historias_lidas:\n",
        "            arquivo_txt_lidas.write(historia + \"\\n\")\n",
        "    print(f\"Arquivo TXT com as histórias utilizadas salvo em: {TXT_LIDAS}\")\n",
        "else:\n",
        "    print(\"Não houve histórias de usuário lidas para salvar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcln_-EC747b",
        "outputId": "30b1fb6b-dac8-43f0-987d-971e44d9ea03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo TXT com as histórias utilizadas salvo em: /content/drive/My Drive/TCC 2025/Base_de_dados/historias_utilizadas_sprint_final.txt\n"
          ]
        }
      ]
    }
  ]
}